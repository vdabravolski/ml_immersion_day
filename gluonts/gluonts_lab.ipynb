{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis with MXNet GluonTS\n",
    "\n",
    "## Problem\n",
    "A **\"forecast\"** is probablistic prediction of certain events in the future based on the past events. Forecasting is a common statistical and machine learning problem which is applied to variety of use cases (from supply/demand prediction to weather forecasts to financial forecasting). When doing forecasting of time-series data you need to consider following:\n",
    "- How many previous events you want to consider when doing forecast (prediction context)?\n",
    "- How far in the future you want to forecast (forecasting horizon);\n",
    "- Is dataset used for forecasting univariate (i.e. single temporal variable available) or multivariate (i.e. several temporal variables available)?\n",
    "- Are there any non-temporal features available?\n",
    "\n",
    "In this lab, we'll predict future consumption of electricity for individual households based on historical data using [Electricity Load dataset](https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014). We'll use GluonTS toolkit and DeepAR model for it. we'll leverage Sagemaker Training and Hyperparameter Optimization (optional) for it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "- Skills: Familiarity with MXNet, Gluon, Python;\n",
    "- Resource: Sagemaker Jupyter notebook, permissions to run Sagemaker distributed training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## GluonTS Toolkit\n",
    "[Gluon Time Series](http://gluon-ts.mxnet.io/) (GluonTS) is Gluon toolkit for probabilistic time series modeling, focusing on deep learning-based models. GluonTS provides utilities for loading and iterating over time series datasets, state of the art models, and building blocks to define your own models and to quickly experiment. With GluonTS you can:\n",
    "- Train and evaluate any of the built-in models on your own data, and quickly come up with a solution for your time series tasks.\n",
    "- Use the provided abstractions and building blocks to create custom time series models, and rapidly benchmark them against baseline algorithms.\n",
    "\n",
    "## DeepAR Algorithm\n",
    "\n",
    "The Amazon SageMaker DeepAR forecasting algorithm is a supervised learning algorithm for forecasting scalar (one-dimensional) time series using recurrent neural networks (RNN). RNN is a class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs.\n",
    "\n",
    "Find DeepAR model architecture below ([source](https://arxiv.org/pdf/1704.04110.pdf)):\n",
    "\n",
    "<center><img src='images/deepar.png' width=700></center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install GluonTS\n",
    "\n",
    "Let's start by installing **gluonts** package. \n",
    "\n",
    "Please note, that if you see error with gluonts import after its successful installation, please restart your kernel (so it \"pick up\" a newly installed package) and retry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gluonts\n",
    "! pip install gluonts\n",
    "! pip install sagemaker==1.72.0\n",
    "\n",
    "# test that it's installed\n",
    "import gluonts\n",
    "import sagemaker\n",
    "print(gluonts.__version__)\n",
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MXNet/GluonTS imports\n",
    "import gluonts\n",
    "from gluonts.dataset.repository.datasets import get_dataset, dataset_recipes\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.deep_factor import DeepFactorEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "from gluonts.dataset.util import to_pandas\n",
    "import sagemaker\n",
    "import mxnet as mx\n",
    "\n",
    "# Standard imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GluonTS built-in datasets\n",
    "\n",
    "GluonTS provides convenient mechanisms to manipulate time-series datasets. It also comes with number built-in datasets. Let's start by exploring available datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Available datasets: {list(dataset_recipes.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GluonTS DataSet object consist of 3 members:\n",
    "- **dataset.train** is an iterable collection of data entries used for training. Each entry corresponds to one time series.\n",
    "- **dataset.test** is an iterable collection of data entries used for validation. The test dataset is an extended version of the train dataset that contains a window in the end of each time series that was not seen during training. This window has length equal to the recommended prediction length.\n",
    "- **dataset.metadata** contains metadata of the dataset such as the frequency of the time series, a recommended prediction horizon, associated features, etc.\n",
    "\n",
    "Let's select _electricity_ dataset and explore it's properties. \n",
    "**Please note**, it will take a few minutes to download dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take ~3-5 minutes\n",
    "dataset_name = 'electricity'\n",
    "data_dir = 'gluonts_data' # define local repository where you'd like to store datasets.\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "data_path = Path(os.getcwd()+\"/\"+data_dir)\n",
    "    \n",
    "dataset = get_dataset(\"electricity\", regenerate=False, path=data_path)\n",
    "print(dataset.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Electricity Dataset\n",
    "\n",
    "Now, let's review test and training time series of electricity dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# get the first time series in the training set\n",
    "train_entry = next(iter(dataset.train))\n",
    "train_entry.keys()\n",
    "\n",
    "# get the first time series in the test set\n",
    "test_entry = next(iter(dataset.test))\n",
    "test_entry.keys()\n",
    "\n",
    "test_series = to_pandas(test_entry)\n",
    "train_series = to_pandas(train_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only last 48 and 72 hours for train and test respectively\n",
    "train_48 = train_series[-48:]\n",
    "test_72 = test_series[-72:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, sharex=True, sharey=True, figsize=(10, 7))\n",
    "\n",
    "train_48.plot(ax=ax[0])\n",
    "ax[0].grid(which=\"both\")\n",
    "ax[0].set_ylabel('kWh')\n",
    "ax[0].legend([\"train series\"], loc=\"upper left\")\n",
    "\n",
    "test_72.plot(ax=ax[1])\n",
    "ax[1].axvline(train_series.index[-1], color='r') # end of train dataset\n",
    "ax[1].grid(which=\"both\")\n",
    "ax[1].set_ylabel('kWh')\n",
    "ax[1].legend([\"test series\", \"end of train series\"], loc=\"upper left\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f\"Length of forecasting window in test dataset: {len(test_series) - len(train_series)}\")\n",
    "print(f\"Recommended prediction horizon: {dataset.metadata.prediction_length}\")\n",
    "print(f\"Frequency of the time series: {dataset.metadata.freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset to S3\n",
    "\n",
    "As we will leverage Amazon Sagemaker for training and inference, it's more convenient and time efficient to store dataset in shared S3 bucket. Code below uploads Electicity data set to specified S3 bucket.\n",
    "\n",
    "## LAB INSTRUCTION\n",
    "\n",
    "Update **bucket**, **region**, and **prefix** variables below. Please make sure that:\n",
    "- bucket has unique name and doesn't exist;\n",
    "- use region where your notebook instance is deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "session=Session()\n",
    "\n",
    "bucket=session.default_bucket()\n",
    "\n",
    "prefix = \"gluonts-data\"\n",
    "region=session.boto_region_name\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from sagemaker.session import Session\n",
    "\n",
    "# bucket creation is slightly different for us-east-1 vs other regions\n",
    "if region==\"us-east-1\":\n",
    "    os.environ[\"AWS_DEFAULT_REGION\"] = region\n",
    "    s3_client = boto3.client('s3')\n",
    "    s3_client.create_bucket(Bucket=bucket)\n",
    "else:\n",
    "    s3_client = boto3.client('s3', region_name=region)\n",
    "    location = {'LocationConstraint': region}\n",
    "    s3_client.create_bucket(Bucket=bucket,\n",
    "                            CreateBucketConfiguration=location)\n",
    "\n",
    "s3_bucket_resource = boto3.resource('s3').Bucket(bucket)\n",
    "s3_data_uri = \"s3://{}/{}\".format(bucket, prefix) # composing URI for bucket above\n",
    "\n",
    "# Define local files for S3 upload\n",
    "local_metadata = os.path.join(data_path, dataset_name, 'metadata.json')\n",
    "local_train_file = os.path.join(data_path, dataset_name, 'train','data.json')\n",
    "local_test_file = os.path.join(data_path, dataset_name, 'test','data.json')\n",
    "\n",
    "s3_metadata = os.path.join(prefix, 'data', 'metadata.json')\n",
    "s3_bucket_resource.Object(s3_metadata).upload_file(local_metadata)\n",
    "\n",
    "s3_train_file = os.path.join(prefix, 'data', 'train', 'data.json')\n",
    "s3_bucket_resource.Object(s3_train_file).upload_file(local_train_file)\n",
    "\n",
    "s3_test_file = os.path.join(prefix, 'data', 'test', 'data.json')\n",
    "s3_bucket_resource.Object(s3_test_file).upload_file(local_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm that data was actually uploaded to S3\n",
    "for my_bucket_object in s3_bucket_resource.objects.all():\n",
    "    print(my_bucket_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker training\n",
    "\n",
    "Sagemaker provides fully managed training and inference ecosystem. Read about it [here](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-training.html). In this lab, we'll learn how to use Sagemaker managed capabilities to train and use at inference time for trained GluonTS models.\n",
    "\n",
    "GluonTS is a part of MXNet ecosystem, so we'll use Sagemaker [MXNet container](https://github.com/aws/sagemaker-mxnet-container) for training and inference. The Amazon SageMaker Python SDK MXNet estimators and models and the Amazon SageMaker open-source MXNet container make writing a MXNet script and running it in Amazon SageMaker easier.\n",
    "\n",
    "To successfully schedule a Sagemaker training job, we need to specify a training script which will be executed on remote Sagemaker nodes. Execute cell below to review already prepared training script. Read [this article](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#prepare-an-mxnet-training-script) to learn more about requirements for MXnet training scripts.\n",
    "\n",
    "Please note following key blocks related to training process:\n",
    "- `if __name__ == \"__main__\"` - this code block is executed when we run our training code as script via `python code.py` command. In this block we parse training hyperparameters, call methods to initiate model training and evaluation;\n",
    "- `train()` method implements main training logic;\n",
    "- `evaluate()` method performs evaluation of trained model on test dataset;\n",
    "- `save_trained_model()` methods saves trained model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize src/gluonts_training_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once we have understanding about our training & inference script, let's schedule our first Sagemaker training job.\n",
    "\n",
    "## LAB INSTRUCTIONS\n",
    "\n",
    "Define following parameters of distributed training:\n",
    "- set **train_instance_type** as **'ml.p3.2xlarge'** - this is an instance type Sagemaker will use for training;\n",
    "- set **train_instance_count** as **1** - this is a training script Sagemaker will run on training node(s);\n",
    "- set **source_dir** as **'src'** - Sagemaker will upload content of this folder to training node(s);\n",
    "- set **entry_point** as **'gluonts_training_script.py'** - Sagemaker will use this script to run training & inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_instance_type =   # follow instructions above\n",
    "train_instance_count =  # follow instructions above\n",
    "source_dir =            # follow instructions above\n",
    "entry_point=            # follow instructions above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sagemaker allows you to define specific metrics which will be scraped from training logs and displayed in training jobs console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics which we want Sagemaker to capture during training\n",
    "metric_definitions = [{'Name': 'final-loss', 'Regex': 'Final loss: ([0-9\\.]+)'},\n",
    "                      {'Name': 'avg-epoch-loss', 'Regex': 'avg_epoch_loss=([0-9\\.]+)' },\n",
    "                      {'Name': 'MASE', 'Regex': '\\'MASE\\': ([0-9\\.]+)'}, # mean absolute scaled error - mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast\n",
    "                      {'Name': 'sMAPE', 'Regex': '\\'sMAPE\\': ([0-9\\.]+)'}, # symmetric mean absolute percentage error\n",
    "                      {'Name': 'MSE', 'Regex': '\\'MSE\\': ([0-9\\.]+)'},\n",
    "                      {'Name': 'MAE_Coverage', 'Regex': '\\'MAE_Coverage\\': ([0-9\\.]+)'}, \n",
    "                      {'Name': 'RMSE', 'Regex': '\\'RMSE\\': ([0-9\\.]+)'}, #root mean square error\n",
    "                      {'Name': 'NRMSE', 'Regex': '\\'NRMSE\\': ([0-9\\.]+)'}, #root mean square error                      \n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define some hyperparameters of DeepAR model\n",
    "\n",
    "## LAB INSTRUCTIONS\n",
    "\n",
    "Define following parameters of distributed training:\n",
    "- set **epochs** to **10**;\n",
    "- set **num-cells** to **20** - this is number of units;\n",
    "- set **num-layers** to **2** - this is number of RNN layers;\n",
    "- set **dropout** to **0.2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "              'epochs':     , # follow instructions above\n",
    "              'num-cells':  , # follow instructions above\n",
    "              'num-layers': , # follow instructions above\n",
    "              'dropout':      # follow instructions above\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.session import Session\n",
    "from random import randint # to generate random ids for sagemaker jobs. TODO add some hashing function that guarantees that the won't be duplicates\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "mxnet_estimator = MXNet(entry_point= entry_point    ,# follow instuctions above\n",
    "          source_dir= source_dir          ,# follow instuctions above  \n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=train_instance_count,\n",
    "          train_instance_type=train_instance_type,    \n",
    "          hyperparameters=hyperparameters,\n",
    "          input_mode='File',\n",
    "          train_max_run=7200,\n",
    "          metric_definitions=metric_definitions,\n",
    "          framework_version='1.6.0')\n",
    "\n",
    "mxnet_estimator.fit(inputs=s3_data_uri, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating training results\n",
    "\n",
    "As part of our training job, we also evaluated trained model against unseen test data (3 different samples) and plotted the forecasts along with historical data. Sagemaker uploads all training job artifacts to S3. Let's retrieve training artifacts from S3 and review them. \n",
    "\n",
    "**Please note**, that charts below are rendered for 3 different data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse # We'll use this package to parse S3 URI\n",
    "import boto3\n",
    "\n",
    "# Works for default S3 location for Sagemaker training job. If you provided custom location, you'll need ot modify it accordingly.\n",
    "job_bucket = mxnet_estimator.latest_training_job.describe()['OutputDataConfig']['S3OutputPath']\n",
    "job_path = mxnet_estimator.latest_training_job.job_name\n",
    "artifact_uri = '{}{}'.format(job_bucket,job_path)\n",
    "uri_object = urlparse(artifact_uri)\n",
    "\n",
    "# Download archive with evaluation artifacts locally\n",
    "archive_filename = \"artifacts.tar.gz\"\n",
    "boto3.client(uri_object.scheme).download_file(uri_object.netloc, \n",
    "                                              \"{}/output/output.tar.gz\".format(job_path), \n",
    "                                              archive_filename)\n",
    "\n",
    "# Untar artifact archive\n",
    "import tarfile\n",
    "tf = tarfile.open(archive_filename)\n",
    "tf.extractall()\n",
    "\n",
    "# Display images which match specific filename pattern \"chart*.png\"\n",
    "from IPython.display import Image, display\n",
    "import fnmatch \n",
    "\n",
    "pattern = 'chart*.png'\n",
    "files = os.listdir('.')\n",
    "\n",
    "listOfImageNames = []\n",
    "\n",
    "for name in files:\n",
    "    if fnmatch.fnmatch(name, pattern):\n",
    "        listOfImageNames.append(name)\n",
    "\n",
    "for imageName in listOfImageNames:\n",
    "    display(Image(filename=imageName, width=800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating forecasting APIs\n",
    "\n",
    "To serve any model using Sagemaker managed inference endpoints, you will need to implement two key components in your training script: loading trained model and model serving. Sagemaker supports following Python methods to implement these components:\n",
    "\n",
    "- **model_fn()** - This method deserialize model artifacts into trained model which will be used during inference. If you don’t provide a model_fn function, the model server uses a default model_fn function. The default function works with MXNet Module model objects saved via the default save function. As in this lab we are using built-in GluonTS model and not MXNet Module model, we'll need to implement custom **model_fn**.\n",
    "\n",
    "- **transform_fn(model, request_body, content_type, accept_type)** - This method request from end user and returns predictions. This one function should handle processing the input, performing a prediction, and processing the output. The return object should be one of the following: \n",
    "    - a tuple with two items: the response data and accept_type (the content type of the response data), OR\n",
    "    - the response data: (the content type of the response is set to either the accept header in the initial request or default to “application/json”)\n",
    "\n",
    "See more details on preparing inference script for Sagemaker endpoint [here](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#for-versions-1-3-and-higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "predictor = mxnet_estimator.deploy(initial_instance_count=1, \n",
    "                                   instance_type=\"ml.c4.xlarge\",\n",
    "                                   endpoint_name=\"gluonts\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "# Prepare data to send over wire for forecasting\n",
    "test_entry = next(iter(dataset.test))\n",
    "test_series = to_pandas(test_entry)\n",
    "json_data = test_series.to_json()\n",
    "\n",
    "# Parameters of Sagemaker predictors\n",
    "predictor.content_type = 'application/json'\n",
    "predictor.deserializer = json_deserializer\n",
    "predictor.serializer = None\n",
    "\n",
    "# Calling predictor\n",
    "result = predictor.predict(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot our predictions along with historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_72 = test_series.tolist()[-72:]\n",
    "predicted_24 = result['predictions']\n",
    "\n",
    "plt.plot(np.arange(72), historic_72)\n",
    "plt.plot(np.arange(72, 96), predicted_24)\n",
    "plt.legend(['historic data for 72hr', 'prediction for 24hr'], loc='upper right')\n",
    "plt.ylabel('kWh')\n",
    "plt.xlabel('hours')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, let's delete forecasting endpoint not avoid any extra costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameter Optimization (optional)\n",
    "\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose.\n",
    "\n",
    "Let's schedule tuning job and explore which combination of hypterparameters leads to better results.\n",
    "\n",
    "**Note**: hyperparameter tuning will take significant time. You can manage training time by chaning max_jobs and max_parallel_jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ranges for tunable parameters. Note, some of parameters have logarithmic scale.\n",
    "hyperparameter_ranges = {\n",
    "    'context-length' : IntegerParameter(12, 70),\n",
    "    'dropout' :  ContinuousParameter(0, 0.5),\n",
    "    'num-layers' : IntegerParameter(1, 20, scaling_type='Logarithmic'),\n",
    "    'num-cells' : IntegerParameter(20, 200, scaling_type='Logarithmic'),\n",
    "    'use-static-features': CategoricalParameter([True, False])\n",
    "}\n",
    "\n",
    "# Define metrics which Sagemaker needs to capture during training.\n",
    "metric_definitions = [{'Name': 'final-loss', 'Regex': 'Final loss: ([0-9\\.]+)'},\n",
    "                      {'Name': 'avg-epoch-loss', 'Regex': 'avg_epoch_loss=([0-9\\.]+)' },\n",
    "                      {'Name': 'MASE', 'Regex': '\\'MASE\\': ([0-9\\.]+)'}, # mean absolute scaled error - mean absolute error of the forecast values, divided by the mean absolute error of the in-sample one-step naive forecast\n",
    "                      {'Name': 'sMAPE', 'Regex': '\\'sMAPE\\': ([0-9\\.]+)'}, # symmetric mean absolute percentage error\n",
    "                      {'Name': 'RMSE', 'Regex': '\\'RMSE\\': ([0-9\\.]+)'}, # root mean square error\n",
    "                      {'Name': 'NRMSE', 'Regex': '\\'NRMSE\\': ([0-9\\.]+)'}, # normalized root mean square error                      \n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LAB INSTRUCTIONS\n",
    "- set **objective_metric_name** as 'NRMSE' - normalized root square error. Sagemaker will use this metric to define which combination of hyperparameters is the best;\n",
    "- set **max_parallel_jobs** as 2 - this is number of remote nodes which Sagemaker will use for training;\n",
    "- set **max_jobs** as 10 - this is total number of training jobs and hyperparameter combinations which Sagemaker will execute;\n",
    "- set **'epochs'** as '5' - this number of epochs in each training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name =      # follow instructions above\n",
    "\n",
    "hpo_model = MXNet(entry_point=entry_point,\n",
    "          source_dir=source_dir,\n",
    "          py_version='py3',\n",
    "          role=role, \n",
    "          train_instance_count=1, \n",
    "          train_instance_type=train_instance_type,\n",
    "          hyperparameters={\n",
    "              'epochs' :      , # follow instructions above\n",
    "              'use-static-features' : False,\n",
    "          },\n",
    "          input_mode='File',\n",
    "          train_max_run=7200,\n",
    "          framework_version='1.6.0')\n",
    "\n",
    "\n",
    "hpo_tuner = HyperparameterTuner(hpo_model,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=           , # follow instructions above\n",
    "                            max_parallel_jobs=  , # follow instructions above\n",
    "                            objective_type=\"Minimize\"\n",
    "                           )\n",
    "\n",
    "# HPO training\n",
    "hpo_tuner.fit(inputs=s3_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Lab\n",
    "During this lab you learned following:\n",
    "- how to work preprocess time-series data using GluonTS;\n",
    "- how to use built-in models in GluonTS and DeepAR specifically;\n",
    "- how to do distirbuted training and hyperparameter tuning using Sagemaker.\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/mxnet-1.8-cpu-py37-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
