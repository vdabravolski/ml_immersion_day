{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Processing on AWS\n",
    "\n",
    "In this lab, we will explore how to analyze dataset using available Python packages as well as perform data processing using AWS services. These tasks are typical pre-requisites to start training your ML model.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "For purpose of this lab, we'll use publicly available dataset [NYC Taxi and Limousine Trips](https://registry.opendata.aws/nyc-tlc-trip-records-pds/). As name suggests, this dataset captures trip records by various transportation vendors (Yellow cab, Green cab, Uber ) in 2010s. The dataset is used extensively to analyze transportation patterns in NYC. Full documentation about this dataset is available [here](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n",
    "\n",
    "As different transportation vendors report their trip in different formats, we'll user only datasets reported by Green Cab taxi service to keep things consistent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration with AWS Data Wrangler\n",
    "\n",
    "[AWS Data Wrangler](https://aws-data-wrangler.readthedocs.io/en/latest/what.html) is an open-source Python package that extends the power of Pandas library to AWS connecting DataFrames and AWS data related services (Amazon Redshift, AWS Glue, Amazon Athena, Amazon EMR, Amazon QuickSight, etc). Built on top of other open-source projects like Pandas, Apache Arrow, Boto3, s3fs, SQLAlchemy, Psycopg2 and PyMySQL, it offers abstracted functions to execute usual ETL tasks like load/unload data from Data Lakes, Data Warehouses and Databases.\n",
    "\n",
    "In this lab, we'll use Data Wrangler to do ad-hoc analysis of dataset and relation between features in given dataset.\n",
    "\n",
    "To start working with Data Wrangler, we need to install it first via Python PIP package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_bucket_contents(bucket_resource, match='', size_mb=0):\n",
    "    \"\"\"\n",
    "    This function list all objects in given bucket which\n",
    "    matches specific name pattern and are bigger than defined \n",
    "    object size.\n",
    "    \"\"\"\n",
    "    total_size_gb = 0\n",
    "    total_files = 0\n",
    "    match_size_gb = 0\n",
    "    match_files = 0\n",
    "    objects = []\n",
    "    for key in bucket_resource.objects.all():\n",
    "        key_size_mb = key.size/1024/1024\n",
    "        total_size_gb += key_size_mb\n",
    "        total_files += 1\n",
    "        list_check = False\n",
    "        if not match:\n",
    "            list_check = True\n",
    "        elif match in key.key:\n",
    "            list_check = True\n",
    "\n",
    "        if list_check and not size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            \n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "            objects.append(f\"s3://{bucket_resource.name}/{key.key}\")            \n",
    "            \n",
    "        elif list_check and key_size_mb >= size_mb:\n",
    "            match_files += 1\n",
    "            match_size_gb += key_size_mb\n",
    "            objects.append(f\"s3://{bucket_resource.name}/{key.key}\")\n",
    "            print(f'{key.key} ({key_size_mb:3.0f}MB)')\n",
    "\n",
    "    if match:\n",
    "        print(f'Matched file size is {match_size_gb/1024:3.1f}GB with {match_files} files')            \n",
    "    \n",
    "    print(f'Bucket {bucket_resource.name} total size is {total_size_gb/1024:3.1f}GB with {total_files} files')\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_s3_bucket = \"nyc-tlc\" # public S3 bucket\n",
    "\n",
    "# Let's create S3 bucket object which we'll use to up\n",
    "s3 = boto3.client('s3')\n",
    "s3_resource = boto3.resource('s3')\n",
    "taxi_bucket = s3_resource.Bucket(taxi_s3_bucket)\n",
    "\n",
    "# select all files with matching having \"trip data\" in S3 key and bigger than 1MB\n",
    "objects = list_bucket_contents(taxi_bucket, match=\"green\", size_mb=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll pick first object\n",
    "idx=0\n",
    "\n",
    "# Need to properly capture dataframe object\n",
    "dfcolumns = wr.s3.read_csv(objects[idx], nrows=1)\n",
    "\n",
    "# This method allows to fetch seamless S3 object to your\n",
    "# notebook memory and then work object using Pandas API\n",
    "df = wr.s3.read_csv(objects[idx],\n",
    "                  header = None,\n",
    "                  skiprows = 1,\n",
    "                  usecols = list(range(len(dfcolumns.columns))),\n",
    "                  names = dfcolumns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to preview first 20 records in the dataset\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's check math statistics of individual records: mean, count, min, max etc.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will remove columns which are doesn't have meaningful information for us\n",
    "df = df.drop([\"Store_and_fwd_flag\", \"Ehail_fee\", \"Trip_type \", \"RateCodeID\", \"Payment_type\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualizing trips geography\n",
    "\n",
    "As we have coordinates (latitude & longitude), let's try see if we can understand where our passengers are being picked up and dropped off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(style='ggplot')\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "%matplotlib inline\n",
    "\n",
    "color = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(28,14))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(df.Pickup_longitude,\n",
    "            df.Pickup_latitude,\n",
    "            s=1,alpha=0.1,color='red')\n",
    "\n",
    "plt.ylim([40.60,41.00])\n",
    "plt.xlim([-74.15,-73.70])\n",
    "plt.xlabel('Longitude',fontsize=16)\n",
    "plt.ylabel('Latitude',fontsize=16)\n",
    "plt.title('Pickup Location',fontsize=18)\n",
    "\n",
    "ax1 = fig.add_subplot(122)\n",
    "ax1.scatter(df.Dropoff_longitude,\n",
    "            df.Dropoff_latitude,\n",
    "            s=1,alpha=0.1,color='blue')\n",
    "\n",
    "plt.ylim([40.60,41.00])\n",
    "plt.xlim([-74.15,-73.70])\n",
    "plt.xlabel('Longitude',fontsize=16)\n",
    "plt.ylabel('Latitude',fontsize=16)\n",
    "plt.title('Dropoff Location',fontsize=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Observations\n",
    "\n",
    "As expected, we can see that Manhattan and Downtown Brooklyn are most popular locations for both pickup and dropoff. It's the most dense areas in NYC in terms of population. We can also speculate as these areas has highest real estate prices, so people who live there have means to use cabs frequently. Another popular area on the map is JFK airport (bottom right corner).\n",
    "\n",
    "It's also clear that our chosen Green Can vendor is rarely picking up people in Manhattan Midtown and Downtown areas. Most likely this is because of either because of specifics of their license or deliberate market segmentation between different Taxi vendors. \n",
    "\n",
    "### Analyze Correlation Between Features\n",
    "\n",
    "As a next step, let's review how different features correlate between each other. For this, we'll print correlation plot for all features in our dataset. Correlation allows us to establish level of interdependency between two random variables. Correlation coefficient belongs to [-1, 1] interval, where \"-1\" indicates inverse relation between variables and \"1\" direct direct relationship. Note, correlation doesn't imply causation.  See more details on correlation [here](https://en.wikipedia.org/wiki/Correlation_and_dependence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numtrain = df.select_dtypes(include=[np.number])\n",
    "corr = numtrain.corr()\n",
    "plt.figure(figsize=(20,20))\n",
    "sns.heatmap(corr,vmax=1,square=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Observations\n",
    "\n",
    "We can confirm several expected trends in the data:\n",
    "- strongest positive correlation is between trip distance and total amount paid;\n",
    "- Tolls and rates are correlated with location features (latitudes and longitudes) as you typically pay when crossing bridge or entering tunnel;\n",
    "- There are medium-to-weak correlations between latitudes and longitudes which can be explained by geagraphical positioning of most dense areas of NYC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering at scale with Sagemaker Processing\n",
    "\n",
    "In order to prepare dataset for ML model, typically we will need to perform following data engineering operations:\n",
    "- drop columns which doesn't present interest and records with empty values;\n",
    "- standartize or normalize numerical features (i.e. float or integer data points);\n",
    "- one-hot encode categorical features.\n",
    "\n",
    "\n",
    "### Standarization of numerical features\n",
    "\n",
    "Standartization is a frequent step in data processing and feature engineering. The goal of this process is to ensure that all numerical features have the same scale. Without standartization or normalization, many algorithms may be numerically unstable and influence. See more details on standartization in [this wiki article](https://en.wikipedia.org/wiki/Feature_scaling).\n",
    "\n",
    "**Question**: can you name numerical feature in the NYC Taxi dataset?\n",
    "\n",
    "\n",
    "### One-hot encoding of categorical features\n",
    "\n",
    "Categorical features in ML terminology are non-numerical features which represent possible classes of given variable. Let's take as an example a categorical feature `color` with possible values of \"red\", \"green\", and \"blue\". Without modifications, `color` variable won't be useful for most of ML algorithms as they won't be able to handle it properly. One-hot encoding allows you to convert categorical feature into set of numerical features, so your ML algorithm can use these feature during training process. \n",
    "\n",
    "Here is an example of one-hot encoding for `color` variable:\n",
    "1. Takes original `color` feature;\n",
    "2. For each possible color value creates a new feature, total number of new features are equal to number of possible values in original `color` feature;\n",
    "3. Assign \"0\" and \"1\" to new features based on value from `color`, where \"1\" is assigned when value in `color` matches derived column.\n",
    "\n",
    "<img src=\"images/onehot.png\" width=600/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precondition - upload data to private S3 bucket\n",
    "\n",
    "Let's copy a subset of Taxi dataset (Green Cab rides for 2014 only) to our private bucket to work only with it going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "\n",
    "# Default bucket which will be used by Sagemaker later in the lab\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "# Constructing S3 path to upload Greencab Taxi dataset\n",
    "taxi_s3_path = f\"s3://{bucket}/taxi_data\"\n",
    "\n",
    "# \"!\" executes command line, in the case we are using AWS CLI utility to copy object from public bucket to your private\n",
    "!aws s3 cp \"s3://nyc-tlc/trip data/\" $taxi_s3_path --recursive --exclude \"*\" --include \"*green_tripdata_2014*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Sagemaker Processing\n",
    "\n",
    "In case of NYC Taxi dataset, we have 36 files for Green Cab vendor with total size of ~8GB. If we wish to handle all files in NYC Taxi dataset, we'd need to handle ~300Gb. Running processing in such sizeable datasets on local box will take considerable time.\n",
    "\n",
    "However, this is a perfect use case for Sagemaker Processing!\n",
    "\n",
    "With Amazon SageMaker Processing jobs, you can leverage a simplified, managed experience to run data pre- or post-processing and model evaluation workloads on the Amazon SageMaker platform. A processing job downloads input from Amazon Simple Storage Service (Amazon S3), then uploads outputs to Amazon S3 during or after the processing job.\n",
    "\n",
    "<img src=\"images/proc_1.jpg\"/>\n",
    "\n",
    "\n",
    "## Leveraging Scikit-learn for data processing\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) is industry standard Python package for Machine Learning. It also provides a number of feature to perform data processing. We'll use available implementations for data standartization \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pygmentize feature_processing.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "sklearn_processor.run(code='feature_processing.py', \n",
    "\n",
    "                      inputs=[ProcessingInput(source=taxi_s3_path, destination='/opt/ml/processing/input/data', \n",
    "                                              s3_data_distribution_type='ShardedByS3Key')],                      \n",
    "                      outputs=[ProcessingOutput(output_name='processed_data',\n",
    "                                                source='/opt/ml/processing/output/data')],\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's confirm that our processed files where automatically uploaded to destination S3 bucket\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "s3_output_path = output_config['Outputs'][0]['S3Output']['S3Uri']\n",
    "\n",
    "! aws s3 ls $s3_output_path --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare processed results with initial \n",
    "\n",
    "Let's now compare how data processing changes the underlying statistics of the dataset. For this, we'll take `total_amount` feature from one of original files and from processed files. Then, we'll print its univariate distribution and compare it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"green_tripdata_2014-01.csv\"\n",
    "\n",
    "original_s3_path = f\"s3://nyc-tlc/trip data/{file}\"\n",
    "processed_s3_path = f\"{s3_output_path}/processed_{file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare initial dataset to processed dataset. \n",
    "\n",
    "### Check that numeric features were standartized as expected.\n",
    "\n",
    "For this, we'll take `Trip_distance` feature and compare boxplots in original dataset and processed dataset. Note, how ranges of values changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcolumns = wr.s3.read_csv(original_s3_path, nrows=1)\n",
    "\n",
    "# Fetch object from S3 to memory\n",
    "original_df = wr.s3.read_csv(original_s3_path,\n",
    "                  header = None,\n",
    "                  skiprows = 1,\n",
    "                  usecols = list(range(len(dfcolumns.columns))),\n",
    "                  names = dfcolumns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(original_df.Total_amount, hist=False)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "# dataset = sns.load_dataset(original_df)\n",
    "ax = sns.boxplot(x=original_df[\"Trip_distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcolumns = wr.s3.read_csv(processed_s3_path, nrows=1)\n",
    "\n",
    "# Fetch object from S3 to memory\n",
    "processed_df = wr.s3.read_csv(processed_s3_path,\n",
    "                  header = None,\n",
    "                  skiprows = 1,\n",
    "                  usecols = list(range(len(dfcolumns.columns))),\n",
    "                  names = dfcolumns.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=processed_df[\"Trip_distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that features where one-hot encoded\n",
    "\n",
    "In our processing script we identified single feature `VendorID` as one appropriate for one-hot encoding procedure. Let's confirm that this operations was performed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical feature in original dataset\n",
    "original_df[[\"VendorID\"]].tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoded features in processed dataset\n",
    "processed_df[[\"Vendor_1\", \"Vendor_2\"]].tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
