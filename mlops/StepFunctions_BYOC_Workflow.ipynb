{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StepFunctions Data Science SDK for BYO Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate how to use the StepFunction Data Science SDK to do an end-to-end data science workflow where you bring your own code and deploy a model on SageMaker.\n",
    "\n",
    "The Steps are as follows:\n",
    "\n",
    "\n",
    "1/ Create a Lambda function which launches a CodeBuild job that launches the creation of your Docker container. (Steps for this are included separately)\n",
    "\n",
    "2/ Launch the Lambda function as a Step Functions workflow. \n",
    "\n",
    "3/ Once the Docker container is built, launch a SageMaker training job using SF DS SDK.\n",
    "\n",
    "4/ Use the DS SDK to deploy the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Add a policy to your SageMaker role in IAM\n",
    "\n",
    "**If you are running this notebook on an Amazon SageMaker notebook instance**, the IAM role assumed by your notebook instance needs permission to create and run workflows in AWS Step Functions. To provide this permission to the role, do the following.\n",
    "\n",
    "1. Open the Amazon [SageMaker console](https://console.aws.amazon.com/sagemaker/). \n",
    "2. Select **Notebook instances** and choose the name of your notebook instance\n",
    "3. Under **Permissions and encryption** select the role ARN to view the role on the IAM console\n",
    "4. Choose **Attach policies** and search for `AWSStepFunctionsFullAccess`.\n",
    "5. Select the check box next to `AWSStepFunctionsFullAccess` and choose **Attach policy**\n",
    "\n",
    "If you are running this notebook in a local environment, the SDK will use your configured AWS CLI configuration. For more information, see [Configuring the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).\n",
    "\n",
    "Next, create an execution role in IAM for Step Functions. \n",
    "\n",
    "### Create an execution role for Step Functions\n",
    "\n",
    "You need an execution role so that you can create and execute workflows in Step Functions.\n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/)\n",
    "2. Select **Roles** and then **Create role**.\n",
    "3. Under **Choose the service that will use this role** select **Step Functions**\n",
    "4. Choose **Next** until you can enter a **Role name**\n",
    "5. Enter a name such as `StepFunctionsWorkflowExecutionRole` and then select **Create role**\n",
    "\n",
    "\n",
    "Attach a policy to the role you created. The following steps attach a policy that provides full access to Step Functions, however as a good practice you should only provide access to the resources you need.  \n",
    "\n",
    "1. Under the **Permissions** tab, click **Add inline policy**\n",
    "2. Enter the following in the **JSON** tab\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateTransformJob\",\n",
    "                \"sagemaker:DescribeTransformJob\",\n",
    "                \"sagemaker:StopTransformJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:CreateHyperParameterTuningJob\",\n",
    "                \"sagemaker:DescribeHyperParameterTuningJob\",\n",
    "                \"sagemaker:StopHyperParameterTuningJob\",\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:ListTags\",\n",
    "                \"lambda:InvokeFunction\",\n",
    "                \"sqs:SendMessage\",\n",
    "                \"sns:Publish\",\n",
    "                \"ecs:RunTask\",\n",
    "                \"ecs:StopTask\",\n",
    "                \"ecs:DescribeTasks\",\n",
    "                \"dynamodb:GetItem\",\n",
    "                \"dynamodb:PutItem\",\n",
    "                \"dynamodb:UpdateItem\",\n",
    "                \"dynamodb:DeleteItem\",\n",
    "                \"batch:SubmitJob\",\n",
    "                \"batch:DescribeJobs\",\n",
    "                \"batch:TerminateJob\",\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:GetJobRuns\",\n",
    "                \"glue:BatchStopJobRun\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:PutTargets\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:DescribeRule\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTransformJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTuningJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForECSTaskRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForBatchJobsRule\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "3. Choose **Review policy** and give the policy a name such as `StepFunctionsWorkflowExecutionPolicy`\n",
    "4. Choose **Create policy**. You will be redirected to the details page for the role.\n",
    "5. Copy the **Role ARN** at the top of the **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries and upload the training data to S3\n",
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import uuid\n",
    "import json\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket() # or feel free to replace with a bucket of your choosing\n",
    "WORK_DIRECTORY = 'PennFudanPed'\n",
    "key = 'BYO-Mask-RCNN'\n",
    "prefix = '{}/{}'.format(key, WORK_DIRECTORY)\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace the IAM role below with the StepFunctionsWorkflowExecutionRole ARN from the role set up. Generally,\n",
    "# it looks like this:\n",
    "workflow_execution_role = \"arn:aws:iam::622475501042:role/StepFunctionExecutionRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# install StepFunctions SDK\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade stepfunctions\n",
    "!{sys.executable} -m pip install sagemaker==1.71.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "import logging\n",
    "from stepfunctions.steps import (LambdaStep, Retry, Catch, Fail, Chain, TrainingStep, ModelStep, EndpointConfigStep, EndpointStep)\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.template.pipeline import TrainingPipeline\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip PennFudanPed.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the training dataset to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_location = sess.upload_data(WORK_DIRECTORY, bucket=bucket, key_prefix=prefix)\n",
    "print(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Estimator and StepFunctions Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = '{}.dkr.ecr.{}.amazonaws.com/sm-container-maskrcnn:torch'.format(account, region) \n",
    "\n",
    "maskrcnn = sagemaker.estimator.Estimator(\n",
    "                                        image_name=image,\n",
    "                                        role=role,\n",
    "                                        train_instance_count=1,\n",
    "                                        train_instance_type='ml.p2.xlarge', #feel free to modify with your own. A cost estimate is provided in Readme.\n",
    "                                        output_path=\"s3://{}/{}/output\".format(sess.default_bucket(), key),\n",
    "                                        sagemaker_session=sess\n",
    "                                    )\n",
    "\n",
    "maskrcnn.set_hyperparameters(num_epochs  = 1,\n",
    "                             num_classes = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create StepFunction Pipeline\n",
    "\n",
    "**IMPORTANT** Replace the Lambda function name below with the Lambda function created in the Outputs of CloudFormation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_state = LambdaStep(\n",
    "    state_id=\"Calls CodeBuild to Build Container\",\n",
    "    parameters={  \n",
    "        \"FunctionName\": \"lambda-build-docker-maskrcnn\", \n",
    "        \"Payload\": {  \n",
    "           \"input\": \" \"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "lambda_state.add_retry(Retry(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    interval_seconds=15,\n",
    "    max_attempts=2,\n",
    "    backoff_rate=4.0\n",
    "))\n",
    "\n",
    "lambda_state.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=Fail(\"LambdaTaskFailed\")\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_input = ExecutionInput(schema={\n",
    "    'JobName': str, \n",
    "    'ModelName': str,\n",
    "    'EndpointName': str\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = TrainingStep(\n",
    "    'Train Step', \n",
    "    estimator=maskrcnn,\n",
    "    data=os.path.dirname(data_location),\n",
    "    job_name=execution_input['JobName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_step = ModelStep(\n",
    "    'Save model',\n",
    "    model=train_step.get_expected_model(),\n",
    "    model_name=execution_input['ModelName'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Endpoint Config\",\n",
    "    endpoint_config_name=execution_input['ModelName'],\n",
    "    model_name=execution_input['ModelName'],\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_step = EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name=execution_input['EndpointName'],\n",
    "    endpoint_config_name=execution_input['ModelName']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_definition = Chain([\n",
    "    lambda_state,\n",
    "    train_step,\n",
    "    model_step,\n",
    "    endpoint_config_step,\n",
    "    endpoint_step\n",
    "])\n",
    "\n",
    "# Next, we define the workflow\n",
    "workflow = Workflow(\n",
    "    name=\"MyWorkflow-BYOC-MaskRCNN-{}\".format(uuid.uuid1().hex),\n",
    "    definition=workflow_definition,\n",
    "    role=workflow_execution_role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workflow.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.render_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = workflow.execute(\n",
    "    inputs={\n",
    "        'JobName': 'BYOC-Mask-RCNN-{}'.format(uuid.uuid1().hex), # Each Sagemaker Job requires a unique name\n",
    "        'ModelName': 'BYOC-Mask-RCNN-{}'.format(uuid.uuid1().hex), # Each Model requires a unique name,\n",
    "        'EndpointName': 'BYOC-Mask-RCNN-{}'.format(uuid.uuid1().hex) # Each Endpoint requires a unique name,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watch the progress of your workflow here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, display_html\n",
    "while True:\n",
    "    display_html(execution.render_progress())\n",
    "    time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferences\n",
    "\n",
    "Once your model is deployed, you can run inferences using this endpoint by using the SageMaker RealTimePredictor API. Please refer to the existing SageMaker documentation for how to do this.\n",
    "\n",
    "Also to ensure you don't rack up costs, make sure you delete the endpoint once you are done. \n",
    "\n",
    "**Important**: Replace the endpoint name below with your endpoint. To find the name, navigate to the SageMaker Console --> Endpoints and look for the name starting with 'BYOC_Mask-RCNN-*******'. **Make sure that your endpoint is up and running before you proceed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the SageMaker Console to get the endpoint name\n",
    "endpoint_name = 'BYOC-Mask-RCNN-76740894e17311eaaadd03f20d7434a0' # TO DO: REPLACE this with your endpoint\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take an input image and run inference on it.\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "SMALL_IMAGE = (200,200)\n",
    "\n",
    "f = f'{WORK_DIRECTORY}/PNGImages/FudanPed00001.png'\n",
    "Image.open(f).resize(SMALL_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "img = np.array(Image.open(f).resize(SMALL_IMAGE))\n",
    "print(f)\n",
    "imginput = json.dumps(img.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = predictor.predict(imginput)\n",
    "prediction = json.loads(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.asarray(prediction[0][0])*255)).resize(SMALL_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.uint8(np.asarray(prediction[1][0])*255)).resize(SMALL_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Monitoring\n",
    "Here we will re-deploy the model with data capture enabled, and will re-run a set of inferences to show that requests and responses are saved for further analysis and ongoing model refinement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = execution.get_input()['ModelName']\n",
    "s3_capture_path = f's3://{bucket}/{key}/capture'\n",
    "s3_capture_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "print(f'Capture path: {s3_capture_path}')\n",
    "\n",
    "## This is the option in EndpointConfiguration which enable capturing data\n",
    "data_capture_configuration = {\n",
    "    'EnableCapture': True, # flag turns data capture on and off\n",
    "    'InitialSamplingPercentage': 100, # sampling rate to capture data. max is 100%\n",
    "    'DestinationS3Uri': s3_capture_path, # s3 location where captured data is saved\n",
    "    'CaptureOptions': [\n",
    "        {\n",
    "            'CaptureMode': 'Output'\n",
    "        },\n",
    "        {\n",
    "            'CaptureMode': 'Input'\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "ts = strftime('%Y-%m-%d-%H-%M-%S', gmtime())\n",
    "endpoint_config_name = f'{key}-model-monitor-{ts}' \n",
    "print(f'EndpointConfig: {endpoint_config_name}')\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.c5.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'InitialVariantWeight':1,\n",
    "        'ModelName':model_name,\n",
    "        'VariantName':'AllTrafficVariant'\n",
    "    }],\n",
    "    DataCaptureConfig = data_capture_configuration) # This is where the new capture options are applied\n",
    "\n",
    "epc_arn = create_endpoint_config_response['EndpointConfigArn']\n",
    "print(f'Endpoint Config Arn: {epc_arn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "\n",
    "endpoint_name = f'{key}-{ts}'\n",
    "print(endpoint_name)\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print(create_endpoint_response['EndpointArn'])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp['EndpointStatus']\n",
    "print(f'Status: {status}')\n",
    "\n",
    "while status=='Creating':\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp['EndpointStatus']\n",
    "    print(f'Status: {status}')\n",
    "\n",
    "ep_arn = resp['EndpointArn']\n",
    "print(f'Arn: {ep_arn}')\n",
    "print(f'Status: {status}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.RealTimePredictor(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 4):\n",
    "    f = f'{WORK_DIRECTORY}/PNGImages/FudanPed{i:05d}.png'\n",
    "    print(f'Finding walkers in image: {f}')\n",
    "    pil_img = Image.open(f).resize(SMALL_IMAGE)\n",
    "    display(pil_img)\n",
    "    img = np.array(pil_img)\n",
    "    imginput = json.dumps(img.tolist())\n",
    "    result = predictor.predict(imginput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wait 1-2 minutes to ensure that data capture is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=bucket, Prefix=f\"{key}/capture\")\n",
    "capture_files = sorted([capture_file.get('Key') for capture_file in result.get('Contents')], reverse=True)\n",
    "print(f'Found {len(capture_files)} Capture Files:\\n')\n",
    "print('\\n '.join(capture_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "def get_obj_body(obj_key):\n",
    "    return s3_client.get_object(Bucket=bucket, Key=obj_key).get('Body').read().decode('utf-8')\n",
    "\n",
    "def content_summary(jsonl_fn):\n",
    "    capture_file = get_obj_body(jsonl_fn)\n",
    "    indiv_jsons = capture_file.split('\\n')\n",
    "    num_objs = len(indiv_jsons)-1\n",
    "    print(f'JSONL file has {num_objs} JSON objects.')\n",
    "    for i in range(num_objs):\n",
    "        capture_json = json.loads(indiv_jsons[i])\n",
    "\n",
    "        in_mode  = capture_json['captureData']['endpointInput']['mode']\n",
    "        out_mode = capture_json['captureData']['endpointOutput']['mode']\n",
    "\n",
    "        input_data = capture_json['captureData']['endpointInput']['data']\n",
    "        output_data = capture_json['captureData']['endpointOutput']['data']\n",
    "\n",
    "        out_type = capture_json['captureData']['endpointOutput']['observedContentType']\n",
    "        out_encoding = capture_json['captureData']['endpointOutput']['encoding']\n",
    "\n",
    "        inference_time = capture_json['eventMetadata']['inferenceTime']\n",
    "\n",
    "        input_image_data = base64.b64decode(input_data)\n",
    "        input_image = json.loads(input_image_data)\n",
    "\n",
    "        body = base64.b64decode(output_data)\n",
    "        prediction = json.loads(body)\n",
    "    \n",
    "        print(f'{i+1:2}/{len(indiv_jsons)-1:2}) inference time: {inference_time}')\n",
    "        print(f'       in mode : {in_mode}, out mode: {out_mode}')\n",
    "        print(f'       out type: {out_type}, out encoding: {out_encoding}')\n",
    "        print(f'       input image size: {int(len(input_image_data)/1000):,} kb')\n",
    "        print(f'       output size     : {int(len(body)/1000):,} kb')\n",
    "        print(f'       Input image:')\n",
    "        \n",
    "        display(Image.fromarray(np.uint8(np.asarray(input_image))).resize(SMALL_IMAGE))\n",
    "        \n",
    "        num_output_images = len(prediction) - 1\n",
    "        print(f'       Output images ({num_output_images}):')\n",
    "        for j in range(num_output_images):\n",
    "            display(Image.fromarray(np.uint8(np.asarray(prediction[j][0])*255)).resize(SMALL_IMAGE))\n",
    "        \n",
    "num_files = len(capture_files)\n",
    "for f in range(num_files):\n",
    "    print(capture_files[f])\n",
    "    content_summary(capture_files[f])"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
